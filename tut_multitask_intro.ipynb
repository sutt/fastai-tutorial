{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple vision ***Learning Tasks*** with `fastai2`\n",
    "\n",
    "      Introduction & Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Series Summary:**\n",
    "\n",
    "We want to understand the internal representations, and event processing performed in the fastai2 library. To do this (or for any numerical library) we import a well known toy dataset and demonstrate a (*load, process, model, evaluate, predict*) modelling loop. Fastai has traditionally used multiple* datasets to demonstrate its use across learning tasks <description of dataset/task list here>\n",
    "\n",
    "The multiple datasets approach has big benefits. But in this tutorial we'll be using one dataset for all tasks This has several benefits: \n",
    "\n",
    " - allows us to summarize the shared/difference in boilerplate to setup the respective learning tasks.\n",
    "   \n",
    " - creates a more apples-to-apples comparison of the processing steps and internal representations, identifying certain techniques as critical or superfluous do different tasks. For example it's important to scale data predicted from a RegressionBlock, but that can be avoided if using a PointBlock.\n",
    "     \n",
    " - we explore our data, its transformation, and underlying model only once, and in detail, saving us overhead of task switching on datasets. However each task will create different evaluation priorities, requirements, and output formats and this will let compare in what ways they can be handled.\n",
    " \n",
    "**In essence**, this exercise is really about what happens when we change the learning task which manifest as changing the dimensions of data type of Y, and the loss function. Although most of vision is about applying transformations, augmentations, etc to X, or optimizer or architecutre, here we focus on the humbler changes, but those which give statistical learning it's power to be applied in any type of situation.\n",
    "\n",
    "<u>Contents:</u>\n",
    "\n",
    "**In part 1**: We'll setup the datablocks, dataloaders and learners necessary to perform 5 separate tasks, and study the encoding/decoding that takes place between each of these steps.\n",
    "\n",
    "Creating these contrived targets (like coords of the top-left-corner of a digit) represent their own toy problem which can help us debug neural network architecture by creating some extrmely simple learning task that can be perfectly represented by a terse basis function, e.g. the coordinates for the top-left point of a mnist-digit. We see powerhouse, but out-of-the-box, ResNets struggle with this task and propose a hand-built simpler architecure to solve these problems. \n",
    "\n",
    "**In part 2**: we'll look at fitting to scalars, and how scaling the data is neccesary. We'll also compare NN vs Linear Regression and see the benefits of the simpler model.\n",
    "\n",
    "\n",
    "**In part 3:** We'll take a detour to breakdown model evaluation into detailed evaluation of what the baseline error rate should be, and how to evaluate the residual distribtuion. \n",
    "\n",
    "**In part 3b:** We break into the headpose dataset to study evaluation graphics for tasks where the targets are points in the image.\n",
    "\n",
    "This lets us look at parameters changes at different layers, adding/removing heads, and doing transfer learning across tasks. These are complicated subjects and so the simplicity of the dataset and target help us here.\n",
    "\n",
    "**In part 4:** We get into the above\n",
    "\n",
    "Finally, we'll show some training routines for each of the tasks, compare perfromance across tasks, and challenge the reader to improve on model-accuracy, architecture, and training-efficiency.\n",
    "\n",
    "\n",
    "/*: BIWI_HeadPose, Oxford_PetBreeds, COCO, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
