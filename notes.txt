	Code Along
	https://github.com/airctic/IceLiveCoding/blob/main/2020-10-13-Live-Coding.md

		https://colab.research.google.com/github/airctic/icedata/blob/master/notebooks/dev/indoor_objects.ipynb
		

	pip install git+git://github.com/airctic/icevision.git#egg=icevision[all] --upgrade
	hey
10.12


	>gcloud compute ssh --zone us-west1-b jupyter@dlvm-cuda101-vm -- -L 8080:localhost:8080

		this logs in as user=user (not user=jupyter)
		gcloud compute ssh --project fastaipt1 --zone us-west1-b dlvm-cuda101-vm -- -L 8080:localhost:8080

	Install Commands:

	>conda create --name newfastai python=3.7
	>conda activate newfastai
	>conda install pytorch torchvision cudatoolkit=10.1 -c pytorch
	>conda install -c fastai fastai
	>conda install jupyter
	>conda install opencv
	>python -m ipykernel install --user --name newfastai --display-name "newfastai2"
	 	in: /home/jupyter/.local/share/jupyter/kernels/newfastai
		(this didn't work the first time)

	still need: opencv, icevision
	
	(not used: >conda install -c fastai -c pytorch -c anaconda fastai gh anaconda)

	>conda install icevision
	>conda install icedata

	docs:
		https://medium.com/pytorch/pytorch-xla-is-now-generally-available-on-google-cloud-tpus-f9267f437832
			https://github.com/pytorch/xla/tree/master/contrib/colab
			https://www.kaggle.com/tanlikesmath/the-ultimate-pytorch-tpu-tutorial-jigsaw-xlm-r
			https://news.ycombinator.com/item?id=24720681
		
		https://cloud.google.com/ai-platform/deep-learning-vm/docs/pytorch_start_instance

		https://console.cloud.google.com/dm/deployments/details/dlvm-cuda101?project=fastaipt1

	extra docs:

		https://aws.amazon.com/blogs/machine-learning/serving-pytorch-models-in-production-with-the-amazon-sagemaker-native-torchserve-integration/

		https://colab.research.google.com/notebooks/tpu.ipynb#scrollTo=MKFMWzh0Yxsq

10.6

	Code along
	https://colab.research.google.com/github/ai-fast-track/icedata/blob/master/Creating_Dataset.ipynb#scrollTo=5x_PcSy4lOJv

	Setting up Cloud
	gcloud compute ssh --zone=us-central1-a jupyter@may2-instance-1 -- -L 8080:localhost:8080
	gcloud compute ssh --zone=us-west1-a jupyter@demo-cuda101 -- -L 8080:localhost:8080
	gcloud beta compute ssh --zone "us-west1-a" "demo-cuda101" --project "fastaipt1" -- -L 8080:localhost:8080

	Get kernels in notebooks
	https://forums.fast.ai/t/google-cloud-config-issue-jupyter-loads-wrong-python-kernel/49598
	https://ipython.readthedocs.io/en/latest/install/kernel_install.html

	python -m ipykernel install --user jupyter --name newfastai --display-name "newfastai"
	python -m ipykernel install --user --name newfastai --display-name "newfastai"

	Building instances
	https://cloud.google.com/ai-platform/deep-learning-vm/docs/release-notes

	Updating / installing cuda drivers on VM's
	https://towardsdatascience.com/installing-cuda-on-google-cloud-platform-in-10-minutes-9525d874c8c1
		http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/

	Narrative Summary:
		
		One problem I've focused on is fitting a NN with many superfluous columns
		 when only a few are highly relevant [actually perfect predictors].

		To illustrate this issue, I've built replicate-data-1.ipynb where I use 
		sklearn.make_regression to create an example dataset to match my problem:
			Y : (-2000,2000)
			N = 700
			p = 804
			feats 0-799: can get 90% R2 via LinReg
			feats 800-804: perfectly linearly correlated with Y

		Running the synthetic regression data through Tabular Model sees the following:

			- when only selecting 20 superfluous columns, we get to R2 .99 in 10 epochs

			- when selecting all 800 superfluous columns, we plateau at .40 -.60 R2

		Then I brought out FeatsNet again. This is where I initially had problems 
		fitting the data with all the feats (feats=['pix', 'pts11',...]).

			But now that I run it again, I'm getting up to .95 R2 
			(albeit with 120 epochs). I thought it was worse than that. 
			In metrics-6b.ipynb
				
			Similar to other work where we took r2 from .95 to .995 viz normalizing Y, 
			this might be the case here too. This work is detailed in:
				 metrics-5b.ipynb - FeatsNet reach r2 of .96 with y un-norm'd

			
			One question is how to make this converge more quickly.

			Issues / Thoughts:

				- One major difference is that 
					FeatsNet has 2-targets, 
					Tabular has  1-target
				
				- The major diff between tabular vs featsnet is feats net has a:
					simpler and shallower architecture.
					- perhaps that's the ley to this type of "low-dimensional" problem?
			
				- I wonder if it has to do with scaling?
					in my synth data, Y is on 1000's scale

				- currently FeatsNet must use image dataloader and tabular must use tabular dl
					so we can't compare them directly yet.
					- need to be able to build a tabular arch in fastai framework 
					  to replicate the FeatsNet simple arch

10.5

	[x] Commit exisitng work in tut
	[x] Replicate or save the dataset
		[x] using sklearn.synthetic_data
		[x] just save to a csv?
		[x] explore the exisiting dataset, e.g. correlations
	[ ] Get a Collab account
		[ ] post the motivating example via collab
	[ ] Post the problem to the forum with writeup
		[ ] collab notebook with all the steps
	
	[ ] Icevision ready to go:

	PR: [ ] It would be also nice to append to csvlogger without duplicating the column headers	


10.4

	[x] Fix CSVLogger bug
		[x] repro ->tut_multitask/cvslog-1.ipynb

		It's in exiting the Context Manager in get_preds that after_epoch is fired.
			specifically in Learner.__exit__

		The learner as a callback get's added in GatherPredsCallback
		or in: ctx_mgrs = self.validation_context(cbs=L(cbs)+[cb], inner=inner)
			in here:

		[x] turn off threading / "multiprocessing"
			[x] verify it is threading currently -> task manager shows 33 - 50% cpu usage
			[x] serach documentation
			[x] verify this on debugger
			num_workers=0 
			n_workers=0

		[x] get a synth learner working
			[x] where in the documentation -> test_utils

		[x] does this work for .validate?

		Solution:
			if hasattr(self, "gather_preds"): return

			at top of both: before_fit and after_fit

		examples to learn from:
			
			class ShowGraphCallback(Callback):
				self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, "gather_preds")

			class FetchPredsCallback(Callback):
			def after_validate(self):
        		to_rm = L(cb for cb in self.learn.cbs if getattr(cb, 'remove_on_fetch', False))
        		with self.learn.removed_cbs(to_rm + self.cbs) as learn:


		[x] git blame on the line in context manager

			-> d9ed4a8337bab36d3680fd787494e83ebd2f9a4b
				by jph, aug 14

				previous commit:
				59d878d3cf233ea24eb8fd8987098f17edd8c8ef

			Need to change torchcore as well:
			
				env=fastai2
					
				  torchcore
					location: fastai2-dev/fastcore-win/
					version:  0.1.11

				  fastai2

					pip-version: 0.0.26
					
					git-version:
					git last commit: May 24
					setting.ini:    0.0.18

						(must set path this to the git repo)
	

		[x] reset the git workspace
		
		This is what we want but it's already off after Learner.__exit__ calls after_epoch

			@contextmanager
    		def no_logging(self): return replacing_yield(self, 'logger', noop)

			This is suppoed to stop CSVLogger from _write_line() by suppressing 
			  the functionality in Recorder in after_epoch line 477, which calls:
					self.logger(self.log)
			
			-> perhaps add it as last event?

		calling get_preds(inner=True) makes it work

		Does self._end_cleanup() ever get called in get_preds()?
			-> no

		Question: is my problem that the context gets turned off in wrong order?
							or that it never does anything to begin with?



10.3

	Setup dev env for fastai:

		The instructions I followed:
		https://docs.fast.ai/dev/develop
		https://docs.fast.ai/dev/git

		These are newer(?) instructions
		https://docs.fast.ai/dev-setup

		https://github.com/fastai/fastai/blob/master/CONTRIBUTING.md

		https://docs.fast.ai/dev/test.html

		https://github.com/fastai/fastai/graphs/contributors
			10 commits to be in top10 (I have 2 already)

		https://docs.fast.ai/dev/git#how-to-make-a-pull-request-pr
			
			notes:
			- run hooks!
			- make sure to create branch!
				otherwise, you can't do subsequent PR's; "divereged origin"
				you can reset using step8
			- use nbdev_build_lib
			- use nbdev_diff_nbs
			- push to origin

		major diff in instructions old vs new
			-> overall: workflow will still work
			- new uses the gh pkg
			- new does a fastcore editable install
				- uses: pip install -qe fastcore
				- this can corrected later
			- new: git push -u origin HEAD (for first PR?)
			- new has more concise fork sync

		starting with 10.7GB of space, ended with 6.4
			(maybe delete pip cache afterwards)

		>conda activate devfastai
		
		>conda install pytorch torchvision cudatoolkit=10.2 -c pytorch   
			
			from: https://pytorch.org/get-started/locally/

		>conda list >  conda-list-10.3.2020-v1.txt

			(in fastai-dev/misc/)

		>git clone 

			(in wsl)

		>pip install -e ".[dev]"  (in sutt-dev-fastai)

		> pip list > ()

			(in fastai-dev/misc/)

		[x] do i have `gh` pkg? -> not automatically but i got it
		
		[x] how to make a PR docs?
			[x] how to handle nb_dev?

		[x] run some test commands
		[x] vscode repo just for that repo
			[x] set the python path

	[x] PR - y_names

		[x] check the tabular examples in docs
			- does it get affected? it is categorical so maybe not?
			- does it go down in accuracy?
			

		[x] code fix:
			additional fixes on cont_names
			additional fixes on set(df)?
			do it inline?

		[x] pr message
			[x] link to the docs

		[x] demonstration notebook
			[x] add section for working with the set(obj) line

		[~] need tests

	[x] Find old bug 
		- it was about erasing CSVLogger after get_preds()
		- it's from 9.6 so those notebooks would be relevant




		

10.1

	[x] y_norm on FeatsNet
		[x] Make it a RegressionBlock not a ImagePoints
		[x] Verify it matches after de-scaling
	
	[x] scaling:
		[x] is this caused by RegressionBlock or did I break the underlying?

	[x] commit

	Narrative Summary:
		- I trained a model to use y_norm with featsNet
			- this is because featsNet was at .95 r2 and so was fastTab 
			  untill I used y_norm'ing which put it to .995 r2. Could the 
			  same thing happen for featsNet? 
			  - if so, that would mean the two arch's are equivalent for this
			    task and I could move on
			-> I got it to work that way, proving the above:
				r2: .964 -> .993
				there's still a skw at the tails, but no skew in center anymore

		- So I piped in the Normalization from the outside:
			- Perfroming Point Scaling and Norm'ing outside the dataloader, 
			   on the df itself
			 - Second, involved changing:
				y_block from ImagePoints -> Regression
		
		- First I found good (I think) perf on mse, but I couldn't properly
		  unscale in metrics_df(), so I went trying to fix that.
		  	- so you can't really compare it's perf to FeatsNet2.1122
			  -> evetually I did and got good results

		- Then I found scaling is an issue because we're no longer in -(1,1) space
		  but more like in (~-3.5, ~3.5) space
		  	- it doesn't appear that fastai.Learner(y_range) works?
			- instead I plug my values into the sigmoid func in FeatsNet
		  -> I went with y_range of samples (not extreme and not default)
		  		- with default, you get a sigmoid PvA because it can't fit points 
				  outside of (-1,1) which there are many
				- with extreme you get slow convergence, but no PvA skew

		-> Now, I have shown how to fit a custom built NN with pts features to get 
		  close to .999 r2 on the topleft task. 
		  This means if I can build the pts features I'll be able to fit this as a true, 
		  chainable NN-Block / NN-layer.


	Thoughts:
		Can you re-parametrize the y_range after fitting the support of your samples?
			so in this case
		Can I do matmul for argmin argmax? This would allow gradient

	Notes:
		
		STN
		https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html

			pytorch differentiable indexing
			https://discuss.pytorch.org/t/differentiable-indexing/17647
			https://discuss.pytorch.org/t/indexing-a-variable-with-a-variable/2111/4?u=magz
			https://discuss.pytorch.org/t/differentiable-indexing-with-float-tensors/81848/2
			https://discuss.pytorch.org/t/gradient-zero-for-one-parameter-in-custom-model-resulting-in-no-update/81091/6

			https://arxiv.org/abs/1506.02025


		How can an NN do argmax?
		
			https://datascience.stackexchange.com/questions/22242/sort-numbers-using-only-2-hidden-layers
			
			https://datascience.stackexchange.com/questions/56676/can-machine-learning-learn-a-function-like-finding-maximum-from-a-list

			https://datascience.stackexchange.com/questions/29345/how-to-sort-numbers-using-convolutional-neural-network

			Claims to have built it
			https://github.com/primaryobjects/nnsorting

				watch out tho, did it just memorize the input space?

			
			https://github.com/bmreiniger/datascience.stackexchange/blob/master/56676.ipynb

				based on this:
				https://core.ac.uk/download/pdf/4894171.pdf
			
			Dense Math
			https://epubs.siam.org/doi/pdf/10.1137/15M1053645


9.30

	[x] do a reload on fastai
		[x] switch to the new conda-env / library; do a reload
		-> cane't!

	[x] understand why tabular normalizes non-list inputs
		[x] can use debugging
		[x] how to turn on and off?

		tabular.data.py line19:
			if cont_names is None: cont_names = list(set(df)-set(cat_names)-set(y_names))

			-> doesn't work if y_names is a string instead of a list
			-> this lets the y col proceed as a `cont_names`

		[x] get it to work for a generic example

	[x] then we want to un-normalize the y's to match mse calc on other metrics

	a note where I stand in fitting topleft with fastai.Tabular:
		- I found that normalization boosts my perf from r2 .95 -> .995
			this actually occured by accident due to the procs on y_names bug
		- I believe this means the issue of fitting two targets at once is not a problem
			-> correct
		- I'm still not sure if a restricted feature set is nec. to find a good fit
			-> no, all cols still procues huge problems
		- Ultimately, we're concluding that NN isn't the best way to fit data with 
			low dimensional, low-stochastic relationships, where basic ML excels
		- Key: I'm still not sure how to make FeatsNet match the perf of fastai.Tabular

		- Why does using all cols in fastTab model diverge? 
			- Did we not fit Net3 with all the pix params?

		The bug in fastai demonstrated in bug-y-procs.ipynb
			- is there a way to apply Normalize to y?
			- one challenge is calcing recorder metrics on y's scale, 
				so encoding/decoding yhat/yactual for each batch
				- is there a simple function from mse(y_sc,yh_sc) -> mse(y, yh)?

		metrics-6
		[x] new notebook
			[X] compact setup
			[~] scaling with objects from fastai
			[x] Use scaler
		[x] pass scaler into metrics_df
		[x] metrics to agg
		[x] commit

		metrics-6b
		[x] do a 10 epoch and 40 epoch fit
		[x] restricted feature set
		[x] scaled and un-scaled: y / x
		[x] commit

		random dev
		[ ] notebook formatting
		[ ] utilities across projects
			[ ] sklearn and fastai metrics
			[ ] pva plots,
				[ ] for points
			[ ] training history helpers
			[ ] grid search utils


	
	[ ] try/except for fastai vs fastai2 imports
		[ ] modules
			[ ] remove modules2
		[ ] how to do it for all notebooks?
			- find and replace?
			- turn off notebook preview?

	[ ] need a new environment for fastai dev with editable installs



9.29

	set ground work for new analysis...
	[x] new nb
	[x] build model in nb
	[x] build features outside of class
	
	[x] use fastai tabular

	[ ] create a synthetic dataset to mimic the situation
		- might need to do correlation among features

	make featsnet2 work
	[x] what if feature set is restricted?
	[x] what if estimate one point at a time?
	[ ] what if hidden layer is removed?
	[ ] what's an easy way to add normalization to custom net?

	notes:
		perhaps the issue is on init?

		When does LR outperform NN's?
		https://arxiv.org/ftp/arxiv/papers/1911/1911.00353.pdf

		MARS outperfroms NN
		https://www.cis.upenn.edu/~ungar/Datamining/Publications/tale2.pdf

		in rcm7  -  we see that the fastai tabular learner fails to accuratley 
					estimate the function, just like BaseNet and FeatsNet

		in rcm7b -  we start to see some clues...
					we restrict the feature set and get better perf
					but we see that there is a difference in fit accuracy for target x vs y
						(as seen in pva plot)
					we then ameliorate this effect by fitting each separately
						(we see .99+ r2_scores)
						problem: now our y becomes normalized over (-3,3)
							[x] what is our mse normalized to regular scores?
							->  less than 1e-3, dist_avg = 1e-3
						Is the added benefit from the normalization? -> yes
					looks like normalization of Y takes us from r2 .95 -> .995+
						(the .95 has a distorted pva plot at the ends)
					[x] if we just normalize both y's, can we fit one model r2 > .99? -> yes

		why can't I edit the intall?


9.28

	[x] FeatsNet, best train
		[x] make it terse
		[x] output metrics

	[x] bring feats pts11, pts12 to the basic-ML models
		[x] metrics on those

	update project overview:
	[x] metrics from metrics-5 saved
		[x] topleft
		[~] center
	[x] metrics from metrics-4 saved
		[x] do both points
		
	
	notes:
		
		+ would there be a better fit for using sqroot loss?
			- what type of synthetic problem could you construct here?
			- this just involves taking a sqroot at the end 
			   before doing loss.backward()?

		+ if basic ML model fit the feats much better, why is my NN model arch so poor?
			Indeed there are features that match target completely

			Maybe I just need a better optimizer, momentum, etc?

9.26

	audit calcs
		- the mse/r2 relationship might not be perfect because diff splits, 
			(at least between sklearn and pytorch models)
			[x] check the baseline err for each
		
	[x] is r2 symmetric? -> no
	
	[x] correct order of pred, actual on r2_score
		[x] twice in mnist_metrics
		[x] anywhere else?
	
	[x] add target to mnist_metrics
	
	[x] rerun metrics 1 and 3
	[x] concat all tables
			
	[x] why is dist_r2 negative for topleft resnet model?
		[x] why is it positive for net3 with same dist_avg?
		-> mistake in calc of baseline
		
	[x] plot dist_r2 vs dist_avg
		-> looks like there's a difference in split
		-> it's still not looking right

	[x] why is mse and dist_avg different b/w net3 and resnet18?
		-> taking sqroot of each residual before summing
		-> there's a little Jensen's inequality here, right?

	[x] commit
	
9.25

	tut
		[x] multi-index dataframe
		
		[x] metrics-1: add resnets
			[x] match to their output on training history
			[x] match mse b/w fastai
			
		[x] metrics-2: add base models
			[x] how to handle 2 y scalars in evaluation?
			[x] how to handle unflow field for the calcs? to match mse
			[x] build sk helper function
			
		
		[x] more metrics
			[x] why is r2 weird on metrics3, center ?
			
				note: changing y_range on inference also changes predictive accuracy of predictions (more liberal -> worse acc)
				
				[x] retrain the model and load into metrics-3
					
					re-running with model_fn = ____4.pth
					
					>gcloud compute scp jupyter@may2-instance-1:~/fastai-tutorial/ model/pt3_center_4.pth
					
					[x] what is the cloud conda env to use?
						-> using fastai2-aug-kernel
					
					[x] why does function eda_fig_1 -> RegressionBlock, break?
						-> just a refactor from fastai2
			
			
		remaining mysteries on resnet metrics:
		
			- might one to save these mysteries until we port to new fastai version
			
			[ ] why did my re-run of topleft training loop result in 45 instead 58 r2? esp. when I am using set_seed()?
			
			[ ] why does metrics_df calc differently on cloud and locally with "same model" and dataset? 
				Split? -> no those are matching
				
			[ ] r2 looks fishy in other areas too, why not corresponding with mse?
				- could it be a problem with r2_score()?
				
		[ ] misc
			[ ] re-populate tut_multitask_3 missing cell
			[ ] commit new metrics + tbls
		
		[ ] metrics todos
			[ ] build baseline
			[ ] with baseline we can plot r2 vs mse and see if identity exists?
					
	
		[ ] metrics-4 
			[ ] BaseNet
			[ ] FeatsNet

		
		other ideas:
			[ ] maybe decreasing the batchsize during training featsnet?
			[ ] why is there a good fit for x on topleft and y on topleft?
				- can display pred vs actual with:
					- title showing r2
					- error bars around y = x
			[ ] audit the centerpoint calc, it looks fishy for one of the 7's
			
		[ ] Bring features from featsnet into classic-ml fitting
			[ ] is there a point that's exactly correct?
		
		[ ] change the names of notebook
			10_glossary_subject_v1.ipynb
			20_research_subject_v2.ipynb
	
	
	obj-detect
		[ ] where does the trained model + arch live on filesystem?
		[ ] some kind of bbox utility?
			- can take from fastai or from icevision
	
	
	notes:
		"keypoint" detection - Review of Lit:
				
				COSFIRE filters, Azzopardi 2013				https://www.um.edu.mt/library/oar/bitstream/123456789/8375/1/PAMI2013.pdf
				https://arxiv.org/pdf/1904.00889.pdf
				
				Eklund 2020 - Segmentation of floor plans + keypoints for the corners
				https://www.diva-portal.org/smash/get/diva2:1450823/FULLTEXT01.pdf
				
				R-Cnn 2015	https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2B_004.pdf
				
				Primer on keypoints	https://www.sicara.ai/blog/2019-07-16-image-registration-deep-learning
					keypoints from SURF/SIFT are used fro stiching/registration
					
					Looking at the invariant features of CNN's
					e.g. https://arxiv.org/ftp/arxiv/papers/1904/1904.00197.pdf
							
		docs have a model for this... does it work?
		https://pytorch.org/docs/stable/torchvision/models.html#keypoint-r-cnn
		
		Detectron Repo, a c/np based framework agnositc (?) 	
		https://github.com/zhaoweicai/Detectron-Cascade-RCNN/blob/master/MODEL_ZOO.md
		
			https://arxiv.org/abs/1906.09756
			
		Optical Flow papers and benchmarks
		https://benchmarks.ai/kitti-optical-flow-2015
		
		
		keep fn key locked on	https://www.reddit.com/r/Surface/comments/7wvy3t/can_the_surface_book_2_lock_the_fn_key/
	
		
		

9.24

	[x] do a commit
	
	[ ] how to catch the histroy of a training run?
	
		fastai issue: CSVLogger append=True doesn't work great
		
	[ ] build class to capture training history
	[ ] build a major results notebook
		- save tabular results as jsons, then add a property about where it came from
		
	[ ] port the notebooks to new version of fastai
	
	[ ] It would be interesting to track training loss without dropout
	
	[ ] How does ridge do on topleft task?
	[ ] How do differnet models do with the expanded feature set?
	
		
	
	Notes on ML:
	
		pretrained model zoo
		https://github.com/balavenkatesh3322/CV-pretrained-model
		
		tips + links from competitions
		https://neptune.ai/blog/binary-classification-tips-and-tricks-from-kaggle
		
		https://www.reddit.com/r/computervision/
	
		ping pong self balancing delta robot	https://www.reddit.com/r/computervision/comments/htoumy/ping_pong_ball_stabilization/
		
			Showcase video
			https://www.youtube.com/watch?v=57DbEEBF7sE
			
			Secret Instructables
			https://www.instructables.com/id/XBall-Balancing-PID-System/
			
			The insturctables archived
			http://archive.is/pXzCq
			
			404 Instructables
			https://www.instructables.com/id/Ball-Balancing-PID-System/
			
			Johan Link, 19 year old etudiant in Switzerland
				https://www.youtube.com/channel/UCKqAb79t9iPfma8BsR_C8jA
				https://www.instructables.com/member/Johan+Link/
				https://www.linkedin.com/in/johan-link-aa538318a/
				
			Another Delta bot - based on capcitance positioning	https://www.instructables.com/id/PID-Controlled-Ball-Balancing-Stewart-Platform/
	
		CV team on producthunt
		https://www.producthunt.com/posts/picsell-ia-3
		
		Variational Auto Encoders - for generative nets
			https://jaan.io/what-is-variational-autoencoder-vae-tutorial/	https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf
		
	
9.23

	[x] Redo notebook custom-...-3 with topleft as the target
	[x] Build an import script for setup
	
	[ ] tut_multitask_3 - bottom section is wrong, doesn't account for topleft task
		[ ] Net3 is exceeds resnet on centerpoint but does it on topleft?
		[ ] Part _04.ipynb also does center point instead of topleft
	
	[x] Do a commit
	
	[x] Display the argmax features with multiplots
	
	[ ] Add an optional other dim to BaseNet, it seems to help so much with topleft
	
	[x] bring pts features into FeatsNet
	
	Interesting idea: the threshold of the cutoff could be differentiable
		Could this be done with a RELU? or doing an .add_ with a torch.where
		But this requires the argmax and "matmul-ish" steps also be differential, right?
		
		[ ] do .build_feats and concat after the first layer to see if you can do differences?
	
	[ ] need a relu layer in my BaseNet?
	

9.21

	NN Tutorial
		Priorities
		[ ] Redo notebook custom-...-3 with topleft as the target
		
		
		Thoughts
		[ ] One thing I'd like to see is adding dummy variables
			- either constants or random values
		[ ] Can I add argmax / argmin in later layers too?
		[ ] What exactly do the argmax features do?
			- why aren't the new features exact matches to the synthetic point targets?
				- could they be made so?
		[ ] Could this model be even better with more hidden layers?
	
	SQL Review
	[ ] connect in jupyter to mysql
	[ ] Convert the problem to mysql
	

9.20

Abandon gumbel softmax for now, work on trying to get argmax into the initial layer

	[x] new notebook
	[x] replicate exisiting work
	[x] look at iafoss's solution, how is his differentiable?
		-> it only changes dimensions of the tiles and puts it thorugh two separate networks
	[x] do we need the pprocessing layers in the init?
		-> to update the weights/biases-parameters, yes
		-> to do a forward pass, no
	
	

9.19

How to concat layers?
https://discuss.pytorch.org/t/concatenate-layer-output-with-additional-input-data/20462/2

What does Unpool do?

9.18

Interesting to see how quickly you can converge in a bare bones mm-style NN task of predicting y from an arbitrary task. (from pytroch demo)

Maybe what we want is a custom nn.Module that does argmin / argmax as additional features when coming into the head.

	see:	https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html#sphx-glr-beginner-examples-nn-two-layer-net-module-py

	
	Interesting challenge case: where the 3's look at topleft point and the 7's look at the middle point.
	
	Or extend this further, to all digits in mnist

	
 Combining digit classification and dependant point location as a task would have contributions to the OCR modelling space and even more broadly to the idea of understanding documents and diagrams.
 
	What are examples of OCR datasets?
	
	What are examples of OCR arch's?
	

Two strong claims I am making is that:
1. This task can't be done well with existing backbone + head. 
2. But I am saying some new form of either a head or full-bacbone-and-head does solve this problem well.

In regards to #1, what ways could I be wrong?

	One possibility is that image size is the problem and dimension redcution whem doing convolutional pooling is erasing the information.
	
	Maybe Min/Max Pooling would help my backbone preserve the information?
	
	Another option is is that I need to do lots of finetuning or feezing or re-training the backbone, or something?
	
	Could be some kind of batch/learning rate thing?
	
Embedding Argmax with autograd enabled:

	https://discuss.pytorch.org/t/differentiable-argmax/33020/8
	
	Could I do a set of AvgMinPool with succesive reduction
	https://discuss.pytorch.org/t/predict-a-categorical-variable-and-then-embed-it-one-hot/85555/5
	
	why is argmin differentiable? 	https://stackoverflow.com/questions/54969646/how-does-pytorch-backprop-through-argmax
	
	"Siamese Network"
	
	"embeddings" in the network
	
	
We can check for if resnet has a "bad backbone" (loses information) or just needs a "better head" (in that the backbone is essentially worthless).

	We can flatten the img data initially and then compare that against the feature-map created by backbone...both fed into the same head...that will give us a relevant comparison.
	
What arch uses two pooling layers?
	-> Resnet18 does at the beginning of head: Avg + Max Pooling
	
	
	
Todo:
[x] replicate existing work
[x] add a torch.nn.Module to replicate the layers thing
[ ] do a forward pass on resnet18,
	[ ] examine traditional pooling
[ ] Replicate the argmax() with nn functions
[ ] 
	


9.8

"TopLeft" target is kind of an "arbitrary" point but should be close to what a bounding box is trying to predict. Why so poor?
    - note: that the point (min(cols), min(rows)) is the true topleft corner of the bounding box

Jeremy chops it up
https://www.wandb.com/podcast/jeremy-howard

My heads so far, like - Net3, Relu4, etc - all used flatten as the first step. That's a problem right there.
    maybe I can add on 

[ ] Get a RCNN pytorch model and watch how the head works?

User submitted SSD in a notebook with fastai_v1
https://forums.fast.ai/t/dynamic-ssd-implementation-for-fastai-v1/36161/3
https://github.com/rohitgeo/singleshotdetector
    -> in my folder as ssd_fastai1.ipynb

Official fastai obj-detector (RetinaNet) discussion:
https://forums.fast.ai/t/object-detection-in-fast-ai-v1/29266

    notebook
    https://github.com/fastai/fastai_dev/blob/master/dev_nb/102a_coco.ipynb

Answers to "NN object localization"

    My project doesn't work
    https://stats.stackexchange.com/questions/362480/object-localization-with-cnn

    Tutorial from pytorch, extensive
    https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html
    
    Tutorial from pytorch, smaller
    https://pytorch.org/hub/nvidia_deeplearningexamples_ssd/

    Quick review of main algos
    https://medium.com/machine-learning-bites/deeplearning-series-objection-detection-and-localization-yolo-algorithm-r-cnn-71d4dfd07d5f

    Review of Lit
    https://machinelearningmastery.com/object-recognition-with-deep-learning/

    Review of the Lit
    https://leonardoaraujosantos.gitbook.io/artificial-inteligence/machine_learning/deep_learning/object_localization_and_detection

    Giant blog review of lit
    https://dudeperf3ct.github.io/object/detection/2019/01/07/Mystery-of-Object-Detection/

    Deep step by step guide to object detection
    https://towardsdatascience.com/building-your-own-object-detector-pytorch-vs-tensorflow-and-how-to-even-get-started-1d314691d4ae

    Review of field + some code
    https://medium.com/analytics-vidhya/guide-to-object-detection-using-pytorch-3925e29737b9

    Papers with code Archive
    https://paperswithcode.com/task/object-localization

9.6

[ ] Investigate new version for bug, propose fix
    CSVLogger values get erased after calling get_preds()

